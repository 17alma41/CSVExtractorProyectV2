# CSVExtractorProyect V2

**CSVExtractorProyect V2** is a professional and automated solution for extracting, filtering, and masking contact data (emails, social networks, phones, etc.) from CSV files and websites. The system is designed to be robust, modular, and easy to use, allowing you to resume processes and handle errors intelligently.

---

## üöÄ What does this project do?
- **Cleans and normalizes** input CSV files.
- **Extracts** emails and social networks from websites using advanced scraping and automatic parallelism.
- **Filters** unwanted emails based on configurable exclusion lists.
- **Generates demo files** with masked/anonymous data for sharing without exposing real data.
- **Records the state** of each file and stage, allowing you to resume interrupted processes.
- **Manages errors** and logs in a centralized and professional way.

---

## üìÅ Project Structure

```
CSVExtractorProyectV2/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                # CLI entry point
‚îÇ   ‚îú‚îÄ‚îÄ settings.py            # Path and constants configuration
‚îÇ   ‚îú‚îÄ‚îÄ extractor/             # Extraction and cleaning
‚îÇ   ‚îú‚îÄ‚îÄ cleaner/               # Exclusion filtering
‚îÇ   ‚îú‚îÄ‚îÄ demo/                  # Demo file generation
‚îÇ   ‚îú‚îÄ‚îÄ utils/                 # Utilities and state/log management
‚îú‚îÄ‚îÄ config/                    # Column and exclusion configuration
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ inputs/                # Original CSVs
‚îÇ   ‚îú‚îÄ‚îÄ clean_inputs/          # Normalized CSVs
‚îÇ   ‚îú‚îÄ‚îÄ outputs/               # Scraping results
‚îÇ   ‚îú‚îÄ‚îÄ exclusions_outputs/    # Results after exclusion
‚îÇ   ‚îî‚îÄ‚îÄ demo_outputs/          # Masked demo files
‚îú‚îÄ‚îÄ logs/                      # Execution logs and state
‚îÇ   ‚îú‚îÄ‚îÄ procesamiento.log
‚îÇ   ‚îú‚îÄ‚îÄ status.json
‚îÇ   ‚îî‚îÄ‚îÄ error_log.txt
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îî‚îÄ‚îÄ README_EN.md               # This documentation
```

---

## ‚öôÔ∏è Installation

1. **Clone the repository** and enter the project folder.
2. Install dependencies:
   ```powershell
   pip install -r requirements.txt
   ```
   > **Note:** Make sure you also have `matplotlib` and `Pillow` (these are included in the requirements file).
3. Make sure you have the Selenium driver (e.g., `chromedriver.exe`) in the `/drivers` folder.

---
## ‚ñ∂Ô∏è Usage Instructions

1. Add your `.csv` files to the `data/inputs` folder
2. Run the main script:
  ```bash
    python src/main.py --all
  ```
4. The full flow will follow: `extraction -> filtering -> demos`
5. Each result will be stored in its corresponding folder: `data/clean_inputs -> data/outputs -> data/exclusions_outputs -> data/demo_outputs`

---

## üñ•Ô∏è Quick Usage

### Fully automated flow

```powershell
python src/main.py --all
```

### Main options
- `--all`           Runs the full flow: cleaning ‚Üí scraping ‚Üí exclusion ‚Üí demo
- `--extract`       Only web extraction
- `--filter`        Only exclusion filtering
- `--demo`          Only demo generation
- `--overwrite`     Forces reprocessing of already processed files
- `--test`          Processes only 20 rows per file (test mode)
- `--wait-timeout`  Page wait timeout (seconds)
- `--resume`        Resumes incomplete or failed files/URLs
- `--clean-logs`    Cleans all files in the logs folder (requires confirmation)

### Advanced usage example
```powershell
python src/main.py --all --overwrite --test --wait-timeout 15
```

---

## üìù Extracting information with FicherosDatos

This script scans country folders within a network path, locates Excel files, extracts specific metrics from the "statistics" sheet of each file, associates up to three available JPG images, and generates a summary Excel file with all that information.

- Run it to apply this code and get the data in an Excel file:
```bash
  python src/scripts/ficheros_datos.py
```
- You will get the results in `data/outputs` for review.

---

## üß† Advanced Features

- **Smart resuming:** If the process is interrupted, you can resume exactly where it left off with `--resume`.
- **State control:** The progress of each file and stage is saved in `/logs/status.json`.
- **Error logs:** All errors are recorded in `/logs/error_log.txt` with details.
- **Automatic parallelism:** The system adjusts the number of threads according to your hardware.
- **Flexible configuration:** You can customize columns, exclusions, and order in `/config/txt_config/`.

---

## üìù Customization and configuration
- Edit the files in `/config/txt_config/` to:
  - Remove unnecessary columns
  - Rename columns
  - Define column order
  - Configure email exclusion lists

---

## üõ†Ô∏è Main module structure

- **src/main.py**: Main CLI and flow orchestrator.
- **src/extractor/**: Web extraction, cleaning, and Excel generation.
- **src/cleaner/**: Email filtering by exclusion.
- **src/demo/**: Masking/anonymous for demo files.
- **src/utils/status_manager.py**: State and log management for resuming and error control.

---

## ‚ùì FAQ

- **Can I resume if the process is interrupted?**
  Yes, run the same command with `--resume` and the system will continue where it left off.
- **How do I clean the logs and state to start from scratch?**
  Run: `python src/main.py --clean-logs`
- **Can I customize the scraping?**
  Yes, adjust the parameters via CLI or edit the configuration in `/config/txt_config/`.

---

## üìÑ License
This project is private and its use is restricted to the terms agreed by the owner.

---

